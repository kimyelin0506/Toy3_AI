import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import mutual_info_classif
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.metrics import silhouette_score
import os
from sklearn.decomposition import PCA
"""
ë°ì´í„° ì „ì²˜ë¦¬:

ë°ì´í„°ëŠ” 50% ìƒ˜í”Œë§ë˜ì–´ ì²˜ë¦¬ë©ë‹ˆë‹¤.

height(í‚¤) ì»¬ëŸ¼ì€ í”¼íŠ¸ì™€ ì¸ì¹˜ ë‹¨ìœ„ì—ì„œ ì„¼í‹°ë¯¸í„°ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.

weight(ëª¸ë¬´ê²Œ)ëŠ” íŒŒìš´ë“œ ë‹¨ìœ„ì—ì„œ í‚¬ë¡œê·¸ë¨ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.

rating(í‰ì )ì€ ê²°ì¸¡ê°’ì„ ì¤‘ì•™ê°’ìœ¼ë¡œ ì±„ìš°ê³ , 5ì  ë§Œì ìœ¼ë¡œ ë°˜ì˜¬ë¦¼í•˜ì—¬ rating_5 ì»¬ëŸ¼ì„ ìƒì„±í•©ë‹ˆë‹¤.

body type(ì²´í˜•) ê°™ì€ ë²”ì£¼í˜• ë°ì´í„°ëŠ” ìµœë¹ˆê°’ìœ¼ë¡œ ê²°ì¸¡ê°’ì„ ì±„ì›ë‹ˆë‹¤.

ì›-í•« ì¸ì½”ë”©ì€ body type ì»¬ëŸ¼ì—ë§Œ ì ìš©ë©ë‹ˆë‹¤.

í”¼ì²˜ ìŠ¤ì¼€ì¼ë§:

ì—°ì†í˜• ë°ì´í„°(ëª¸ë¬´ê²Œ, í‚¤, ë‚˜ì´, ì‚¬ì´ì¦ˆ ë“±)ëŠ” MinMaxScalerë¥¼ ì‚¬ìš©í•˜ì—¬ 0ê³¼ 1 ì‚¬ì´ë¡œ ì •ê·œí™”ë©ë‹ˆë‹¤.

ì›-í•« ì¸ì½”ë”©ëœ body type ì»¬ëŸ¼ì€ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ ì˜í–¥ë ¥ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.

ê°€ì¤‘ì¹˜ ìµœì í™”:

PCA(ì£¼ì„±ë¶„ ë¶„ì„)ì™€ MI(ìƒí˜¸ ì •ë³´)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ì›-í•« ì¸ì½”ë”© ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

ì´ë¥¼ í†µí•´ ìµœì ì˜ í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ íŠ¹ì„± ì„ íƒê³¼ ê°€ì¤‘ì¹˜ ì„¤ì •ì„ ìë™ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.

KMeans í´ëŸ¬ìŠ¤í„°ë§:

ìµœì ì˜ Kê°’(í´ëŸ¬ìŠ¤í„° ìˆ˜)ì„ ì°¾ê¸° ìœ„í•´ elbow method(ì—˜ë³´ìš° ë°©ë²•)ì™€ silhouette score(ì‹¤ë£¨ì—£ ì ìˆ˜)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

KMeans í´ëŸ¬ìŠ¤í„°ë§ì„ ì‹¤í–‰í•˜ê³ , í´ëŸ¬ìŠ¤í„°ë§ì˜ ì„±ëŠ¥ì„ ì‹¤ë£¨ì—£ ì ìˆ˜ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”:

KMeans í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ PCA(2D)ë¡œ ì‹œê°í™”í•˜ì—¬ ê° í´ëŸ¬ìŠ¤í„°ì˜ ë¶„í¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

ê° í´ëŸ¬ìŠ¤í„°ì˜ íŠ¹ì„± ì¤‘ì‹¬ì„ ë¶„ì„í•˜ì—¬, ì–´ë–¤ íŠ¹ì„±ì´ í´ëŸ¬ìŠ¤í„° ë¶„í¬ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤.

í´ëŸ¬ìŠ¤í„°ë§ ì„±ëŠ¥ í‰ê°€:

MI ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ê³¼ PCA ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ì˜ ì‹¤ë£¨ì—£ ì ìˆ˜ë¥¼ ë¹„êµí•˜ì—¬, ì–´ë–¤ ë°©ë²•ì´ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ì£¼ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.
"""
# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs('plots', exist_ok=True)

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv('../renttherunway_data.csv')
print(f"ì „ì²´ ë°ì´í„° í¬ê¸°: {df.shape}")

# ğŸ‘‰ ë°ì´í„° 50% ìƒ˜í”Œë§
df = df.sample(frac=0.5, random_state=42).reset_index(drop=True)

# í‚¤ë¥¼ cmë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def height_to_cm(height):
    if pd.isna(height):
        return np.nan
    try:
        feet, inches = height.split("'")
        inches = inches.replace('"', '').strip()
        return int(feet) * 30.48 + int(inches) * 2.54
    except:
        return np.nan

# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocessing_data(data):
    selected_columns = ['user_id', 'item_id', 'fit', 'weight', 'rating', 'body type', 'height', 'size', 'age']
    data = data[selected_columns].copy()
    data = data[data['fit'] == 'fit'].copy()

    data['height_cm'] = data['height'].apply(height_to_cm)
    data['weight'] = data['weight'].astype(str).str.replace('lbs', '').str.strip()
    data['weight'] = pd.to_numeric(data['weight'], errors='coerce')
    data['weight_kg'] = data['weight'] * 0.453592

    data['rating'] = pd.to_numeric(data['rating'], errors='coerce').replace([np.inf, -np.inf], np.nan)
    rating_median = data['rating'].median()
    data['rating'] = data['rating'].fillna(rating_median)
    data['rating_5'] = (data['rating'] / 2).round().astype(int).clip(1, 5)

    numeric_columns = ['height_cm', 'weight_kg', 'rating_5', 'age', 'size']
    for col in numeric_columns:
        data[col] = data[col].fillna(data[col].mean())

    categorical_columns = ['fit', 'body type']
    for col in categorical_columns:
        data[col] = data[col].fillna(data[col].mode()[0])

    # ì›-í•« ì¸ì½”ë”©
    # encoder_fit = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    # fit_encoded = pd.DataFrame(encoder_fit.fit_transform(data[['fit']]),
    #                            columns=encoder_fit.get_feature_names_out(['fit']), index=data.index)

    encoder_body = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    body_encoded = pd.DataFrame(encoder_body.fit_transform(data[['body type']]),
                                columns=encoder_body.get_feature_names_out(['body type']), index=data.index)
    # data = pd.concat([data, fit_encoded, body_encoded], axis=1)
    data = pd.concat([data, body_encoded], axis=1)
    return data

# ìŠ¤ì¼€ì¼ë§ í•¨ìˆ˜
def scaling_data(data, onehot_weight=0.2):
    scaler = MinMaxScaler()
    scaled_columns = ['weight_kg', 'height_cm', 'size', 'age']

    # ì—°ì†í˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§
    for col in scaled_columns:
        data[f'{col}_scaled'] = scaler.fit_transform(data[[col]])

    # ì›í•« ì¸ì½”ë”© ì»¬ëŸ¼ ì°¾ê¸°
    onehot_columns = [col for col in data.columns if col.startswith(('fit_', 'body type_'))]

    # ì›í•« ì¸ì½”ë”© ì»¬ëŸ¼ì— ê°€ì¤‘ì¹˜ ì ìš©
    for col in onehot_columns:
        data[col] = data[col] /7

    # ìµœì¢… feature ëª¨ìŒ
    feature_columns = [f'{col}_scaled' for col in scaled_columns] + onehot_columns
    return data[feature_columns]

def find_auto_onehot_weight(train_df, optimal_k=20):
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score

    # ìŠ¤ì¼€ì¼ë§ + ì—°ì†í˜• ë³€ìˆ˜ í‰ê·  ê³„ì‚°
    scaled_data = train_df.copy()
    scaler = MinMaxScaler()
    scaled_columns = ['weight_kg', 'height_cm', 'size', 'age']

    for col in scaled_columns:
        scaled_data[f'{col}_scaled'] = scaler.fit_transform(scaled_data[[col]])

    # ì—°ì†í˜• ìŠ¤ì¼€ì¼ë§ëœ ì»¬ëŸ¼ë“¤ì˜ í‰ê·  ê³„ì‚°
    feature_means = []
    for col in scaled_columns:
        mean_value = scaled_data[f'{col}_scaled'].mean()
        feature_means.append(mean_value)

    auto_onehot_weight = np.mean(feature_means)
    print(f"\nğŸ“ˆ ìë™ ê³„ì‚°ëœ OneHot ê°€ì¤‘ì¹˜ (ì—°ì†í˜• í‰ê·  ê¸°ë°˜): {auto_onehot_weight:.4f}")

    # ì´ì œ ì´ ê°€ì¤‘ì¹˜ë¡œ ë‹¤ì‹œ ìŠ¤ì¼€ì¼ë§
    onehot_columns = [col for col in scaled_data.columns if col.startswith(('fit_', 'body type_'))]
    for col in onehot_columns:
        scaled_data[col] = scaled_data[col] * auto_onehot_weight

    feature_columns = [f'{col}_scaled' for col in scaled_columns] + onehot_columns
    feature_df = scaled_data[feature_columns]

    # KMeans í•™ìŠµ
    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10, init='k-means++')
    labels = kmeans.fit_predict(feature_df)

    # ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
    silhouette = silhouette_score(feature_df, labels)
    print(f"âœ… ì‹¤ë£¨ì—£ ì ìˆ˜ (auto weight): {silhouette:.4f}")

    return auto_onehot_weight, silhouette

# ë°ì´í„° ë¶„í•  í•¨ìˆ˜
def train_new_data_split(data, test_size, random_state):
    train_data, new_data = train_test_split(data, test_size=test_size, random_state=random_state)
    print(f"train í¬ê¸°: {train_data.shape}, new í¬ê¸°: {new_data.shape}")
    return train_data, new_data

# ìµœì ì˜ kì°¾ì•„ì„œ ì ìš©
def find_optimal_k(train_features_df, k_min=2, k_max=30):
    wcss = []
    silhouette_scores = []

    for k in range(k_min, k_max + 1):
        """
        KMeans(n_clusters=optimal_k,
                random_state=42,
                n_init=10,
                init='k-means++',
                max_iter=500,
                algorithm='elkan',
                tol=1e-5)
        """
        # kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, init='k-means++')
        kmeans = KMeans(n_clusters=k,
               random_state=42,
               n_init=10,
               init='k-means++',
               max_iter=500,
               algorithm='elkan',
               tol=1e-5)
        labels = kmeans.fit_predict(train_features_df)
        wcss.append(kmeans.inertia_)
        if k > 1:  # ì‹¤ë£¨ì—£ ì ìˆ˜ëŠ” k=2 ì´ìƒë¶€í„° ì˜ë¯¸ ìˆìŒ
            silhouette = silhouette_score(train_features_df, labels)
            silhouette_scores.append((k, silhouette))
            print(k,"ì‹¤ë£¨ì—£ ì ìˆ˜: ", silhouette)

    # ì—˜ë³´ìš° í”Œë¡¯ ì €ì¥
    plt.figure(figsize=(10, 6))
    plt.plot(range(k_min, k_max + 1), wcss, marker='o')
    plt.title('Elbow Method for Optimal K')
    plt.xlabel('Number of clusters (K)')
    plt.ylabel('WCSS')
    plt.savefig('plots/elbow_plot.png')
    plt.show()
    plt.close()

    # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ê°€ ê°€ì¥ ë†’ì€ k ì„ íƒ
    best_k, best_silhouette = max(silhouette_scores, key=lambda x: x[1])
    print(f"\nâœ… ìë™ìœ¼ë¡œ ì„ íƒëœ ìµœì  K: {best_k} (Silhouette: {best_silhouette:.4f})")
    return best_k

# --- PCA ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚° ---
def pca_based_onehot_weight(train_df, continuous_cols, onehot_cols):
    # ì—°ì†í˜•, ë²”ì£¼í˜• êµ¬ë¶„
    cont_data = train_df[continuous_cols]
    onehot_data = train_df[onehot_cols]

    # ì—°ì†í˜• ìŠ¤ì¼€ì¼ë§
    scaler = MinMaxScaler()
    cont_scaled = scaler.fit_transform(cont_data)

    # ì›-í•«ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    data_combined = np.hstack([cont_scaled, onehot_data.values])

    # PCA ì ìš©
    pca = PCA(n_components=min(data_combined.shape))
    pca.fit(data_combined)

    # ê° ë³€ìˆ˜ë³„ ê¸°ì—¬ë„
    loading_scores = np.abs(pca.components_).mean(axis=0)  # ê° featureë³„ í‰ê·  loading

    # ì—°ì†í˜•, ë²”ì£¼í˜• ë¶„ë¦¬
    cont_loading = loading_scores[:len(continuous_cols)].mean()
    onehot_loading = loading_scores[len(continuous_cols):].mean()

    # ë¹„ìœ¨ ê³„ì‚°
    weight_ratio = cont_loading / onehot_loading
    print(f"ì—°ì†í˜• í‰ê·  Loading: {cont_loading:.4f}, ë²”ì£¼í˜• í‰ê·  Loading: {onehot_loading:.4f}")
    print(f"ì¶”ì²œí•˜ëŠ” ë²”ì£¼í˜• ê°€ì¤‘ì¹˜ (PCA ê¸°ë°˜): {weight_ratio:.4f}")

    return weight_ratio

# --- MI ê¸°ë°˜ íŠ¹ì„± ì„ íƒ ---
def select_features_by_mi(X, y, top_n=10):
    """
    Mutual Informationì„ ì‚¬ìš©í•´ ê°€ì¥ ì¤‘ìš”í•œ feature nê°œ ì„ íƒ
    """
    # Mutual Information ê³„ì‚°
    mi_scores = mutual_info_classif(X, y, random_state=42)
    mi_df = pd.DataFrame({'feature': X.columns, 'mi_score': mi_scores})
    mi_df = mi_df.sort_values(by='mi_score', ascending=False)

    print("\nğŸ“Š Mutual Information ìƒìœ„ Feature:")
    print(mi_df.head(top_n))

    selected_features = mi_df['feature'].iloc[:top_n].tolist()
    return selected_features

# --- KMeans í´ëŸ¬ìŠ¤í„°ë§ í•¨ìˆ˜ ---
def apply_kmeans_with_silhouette(train_features_df, optimal_k, features_selected=None):
    # ìµœì  í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    if features_selected:
        train_features_df = train_features_df[features_selected]

    kmeans = KMeans(n_clusters=optimal_k,
                    random_state=42,
                    n_init=10,
                    init='k-means++',
                    max_iter=500,
                    algorithm='elkan',
                    tol=1e-5)
    train_df['cluster'] = kmeans.fit_predict(train_features_df)

    # ì‹¤ë£¨ì—£ ì ìˆ˜ í™•ì¸
    silhouette = silhouette_score(train_features_df, train_df['cluster'])
    print(f"\nì„ íƒëœ Feature ê¸°ë°˜ í•™ìŠµ ë°ì´í„° ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette:.3f}")

    return kmeans, silhouette

# --- PCA ê¸°ë°˜ê³¼ MI ê¸°ë°˜ì„ ëª¨ë‘ ì ìš©í•˜ëŠ” í•¨ìˆ˜ ---
def apply_pca_and_mi_based_clustering(train_df, continuous_cols, onehot_cols):
    # 1. PCA ê¸°ë°˜ OneHot ê°€ì¤‘ì¹˜ ê³„ì‚°
    pca_weight = pca_based_onehot_weight(train_df, continuous_cols, onehot_cols)

    # 2. MI ê¸°ë°˜ íŠ¹ì„± ì„ íƒ
    selected_features_mi = select_features_by_mi(train_df[continuous_cols + onehot_cols], train_df['cluster_initial'], top_n=10)

    # 3. PCA ê°€ì¤‘ì¹˜ ì ìš©
    train_features_df_pca = scaling_data(train_df, onehot_weight=pca_weight)

    # 4. MI íŠ¹ì„±ë§Œ ì ìš©í•˜ì—¬ KMeans ìˆ˜í–‰
    optimal_k_mi = find_optimal_k(train_features_df_pca, k_min=8, k_max=21)
    kmeans_mi, silhouette_mi = apply_kmeans_with_silhouette(train_features_df_pca, optimal_k_mi, selected_features_mi)

    # 5. PCA ê¸°ë°˜ KMeans ìˆ˜í–‰
    optimal_k_pca = find_optimal_k(train_features_df_pca, k_min=6, k_max=21)
    kmeans_pca, silhouette_pca = apply_kmeans_with_silhouette(train_features_df_pca, optimal_k_pca)

    return kmeans_mi, silhouette_mi, kmeans_pca, silhouette_pca

# --- ë¦¬íŒ©í† ë§ ì‹¤í–‰ íë¦„ ì‹œì‘ ---
# 1. ë°ì´í„° ë¶„í•  (ì›ë³¸ì—ì„œ ë¨¼ì €!)
train_df_raw, new_df_raw = train_new_data_split(df, test_size=0.01, random_state=42)

# 2. ê°ê° ì „ì²˜ë¦¬
train_df = preprocessing_data(train_df_raw)
new_df = preprocessing_data(new_df_raw)

# ì´ auto_weightë¥¼ scaling_data()ì— ë„˜ê²¨ì„œ ìµœì¢… ë°ì´í„° ì¤€ë¹„
train_features_df = scaling_data(train_df, onehot_weight=0.7989)
new_features_df = scaling_data(new_df, onehot_weight=0.7989)

# best_weightë¥¼ scaling_data()ì— ë„˜ê²¨ì„œ ìµœì¢… ë°ì´í„° ì¤€ë¹„í•˜ê¸°
# train_features_df = scaling_data(train_df, onehot_weight=best_weight)
# new_features_df = scaling_data(new_df, onehot_weight=best_weight)

# 4. ìµœì  í´ëŸ¬ìŠ¤í„° ì°¾ê¸° (ìë™)
optimal_k = find_optimal_k(train_features_df, k_min=6, k_max=21)
# optimal_k = 21

# 5. K-Means í´ëŸ¬ìŠ¤í„°ë§
# optimal_k = 20
# kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10, init='k-means++')
# train_df['cluster'] = kmeans.fit_predict(train_features_df)

# --- ìµœì¢… ì‹¤í–‰ ---
# continuous_colsì™€ onehot_cols ì •ì˜
continuous_cols = ['height_cm', 'weight_kg', 'rating_5', 'age', 'size']
onehot_cols = [col for col in train_df.columns if col.startswith(('fit_', 'body type_'))]

# PCA ë° MI ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì ìš©
kmeans_mi, silhouette_mi, kmeans_pca, silhouette_pca = apply_pca_and_mi_based_clustering(train_df, continuous_cols, onehot_cols)

# 5. K-Means í´ëŸ¬ìŠ¤í„°ë§
def apply_kmeans_clustering(train_df, features_df, optimal_k, kmeans_init='k-means++', n_init=10):
    """
    MI ë° PCA ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ ì ìš©í•œ í›„, í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ë°˜í™˜
    :param train_df: í•™ìŠµ ë°ì´í„°í”„ë ˆì„
    :param features_df: íŠ¹ì„± ë°ì´í„°í”„ë ˆì„ (PCAì™€ MI ê¸°ë°˜)
    :param optimal_k: ìµœì  í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
    :param kmeans_init: KMeans ì´ˆê¸°í™” ë°©ì‹
    :param n_init: KMeans n_init ê°’
    :return: í•™ìŠµëœ KMeans ëª¨ë¸, í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”
    """
    kmeans = KMeans(n_clusters=optimal_k,
                    random_state=42,
                    n_init=n_init,
                    init=kmeans_init,
                    max_iter=500,
                    algorithm='elkan',
                    tol=1e-5)

    # KMeans í´ëŸ¬ìŠ¤í„°ë§
    train_df['cluster'] = kmeans.fit_predict(features_df)

    # ì‹¤ë£¨ì—£ ì ìˆ˜ í™•ì¸
    silhouette = silhouette_score(features_df, train_df['cluster'])
    print(f"í•™ìŠµ ë°ì´í„° ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette:.3f}")

    return kmeans, silhouette


# 6. í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ KMeans ëª¨ë¸ í•™ìŠµ
kmeans_mi_final, silhouette_mi_final = apply_kmeans_clustering(train_df, new_features_df, optimal_k)
kmeans_pca_final, silhouette_pca_final = apply_kmeans_clustering(train_df, train_features_df, optimal_k)

# 7. í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë¶„ì„ (MI, PCA ê¸°ë°˜ ê²°ê³¼ ë¹„êµ)
print(f"MI ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette_mi_final:.3f}")
print(f"PCA ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette_pca_final:.3f}")


# 8. í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™” (ì˜ˆ: PCA ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ 2D ì‹œê°í™”)
def plot_cluster_results(features_df, cluster_labels, title="Cluster Plot"):
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(features_df)

    plt.figure(figsize=(10, 8))
    sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1], hue=cluster_labels, palette='viridis', s=100, alpha=0.7)
    plt.title(title)
    plt.xlabel('PCA 1')
    plt.ylabel('PCA 2')
    plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.show()

# PCA ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”
plot_cluster_results(train_features_df, kmeans_pca_final.labels_, title="PCA ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼")

# MI ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”
plot_cluster_results(new_features_df, kmeans_mi_final.labels_, title="MI ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼")


# 9. í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± ë¶„ì„
def analyze_cluster_centroids(kmeans_model, feature_names):
    print("\ní´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ë¶„ì„:")
    centroids = kmeans_model.cluster_centers_
    cluster_centroids_df = pd.DataFrame(centroids, columns=feature_names)
    print(cluster_centroids_df)


# MI ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ centroids ë¶„ì„
analyze_cluster_centroids(kmeans_mi_final, continuous_cols + onehot_cols)

# PCA ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ centroids ë¶„ì„
analyze_cluster_centroids(kmeans_pca_final, continuous_cols + onehot_cols)