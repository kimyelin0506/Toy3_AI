# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import classification_report, confusion_matrix

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

from pre_women_body_type import X_train, X_test, y_train, y_test, y_encoded, le_body
"""
[ë”¥ëŸ¬ë‹ ì‹ ê²½ë§ ëª¨ë¸(MLP)ì„ ë§Œë“¤ì–´ì„œ, ì‚¬ëŒ ì²´í˜•(body type)ì„ ë¶„ë¥˜í•˜ëŠ” ê²ƒ]
<ì „ì²´ íë¦„ ìš”ì•½>
(1) í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
- numpy: ìˆ˜ì¹˜ ê³„ì‚°ìš©
- matplotlib/seaborn: ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
- sklearn: í‰ê°€ ì§€í‘œ (confusion matrix, classification report)
- keras: ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¶•
- ë¯¸ë¦¬ ì „ì²˜ë¦¬í•´ë†“ì€ X_train, X_test ë“± ë°ì´í„°ì…‹ ê°€ì ¸ì˜´.

(2) ì‹ ê²½ë§ ëª¨ë¸(MLP) ë§Œë“¤ê¸°
- Sequential ëª¨ë¸: í•œ ì¸µì”© ìˆœì„œëŒ€ë¡œ ìŒ“ëŠ” ë°©ì‹
    - ì²« ë²ˆì§¸ Dense ì¸µ: 128ê°œì˜ ë‰´ëŸ°, ReLU í™œì„±í™”
    - Dropout(0.3): 30% ëœë¤ìœ¼ë¡œ ë…¸ë“œ ë„ê¸° (ê³¼ì í•© ë°©ì§€)
    - ë‘ ë²ˆì§¸ Dense ì¸µ: 64ê°œ ë‰´ëŸ°, ReLU
    - ë˜ Dropout
    - ë§ˆì§€ë§‰ Dense ì¸µ: í´ë˜ìŠ¤ ìˆ˜ë§Œí¼ ì¶œë ¥, Softmax (ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜)
    - ìš”ì•½: ì‹¬í”Œí•œ MLP ì‹ ê²½ë§
- ëª¨ë¸ ì»´íŒŒì¼
    - adam: ìµœì í™” ì•Œê³ ë¦¬ì¦˜
    - sparse_categorical_crossentropy: ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ìš© loss í•¨ìˆ˜ (ë ˆì´ë¸”ì´ one-hot encodingì´ ì•„ë‹ˆë¼ ì •ìˆ˜í˜•ì¼ ë•Œ ì‚¬ìš©)
    - metrics=['accuracy']: ì •í™•ë„ í™•ì¸

(3) ëª¨ë¸ í•™ìŠµ (fit)
- early_stopping: validation lossê°€ 5ë²ˆ ì—°ì† ì¢‹ì•„ì§€ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨ (ê³¼ì í•© ë°©ì§€)
- history: í•™ìŠµ ê³¼ì • ì €ì¥ (loss/accuracy ê¸°ë¡)

(4) ëª¨ë¸ í‰ê°€ (confusion matrix, classification report, F1 score)
- ì˜ˆì¸¡ ê²°ê³¼ í™•ë¥  â†’ ê°€ì¥ ë†’ì€ í™•ë¥  í´ë˜ìŠ¤ë¥¼ ì„ íƒ.
- precision / recall / f1-score ì¶œë ¥í•´ì„œ ì„±ëŠ¥ í‰ê°€.
- confusion matrix ì‹œê°í™”: ì–´ë–¤ í´ë˜ìŠ¤ë¥¼ ì˜ ë§ì·„ê³ , í—·ê°ˆë ¸ëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ í™•ì¸.
- weighted F1 score ê³„ì‚°í•´ì„œ ì „ì²´ì ì¸ ëª¨ë¸ ì„±ëŠ¥ í™•ì¸.

(5) ê²°ê³¼ ì‹œê°í™” (loss/accuracy ê·¸ë˜í”„)
- Overfittingì´ ìˆëŠ”ì§€/ì„±ê³µì ìœ¼ë¡œ í•™ìŠµí–ˆëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ í™•ì¸.
"""
"""
1. ì™œ Dropoutì„ 2ë²ˆì´ë‚˜ ë„£ì—ˆì„ê¹Œ?
ğŸ‘‰ ê³¼ì í•©(Overfitting)ì„ ë°©ì§€í•˜ë ¤ê³ .
Dropoutì€ í•™ìŠµí•  ë•Œ ëœë¤í•˜ê²Œ ì¼ë¶€ ë‰´ëŸ°ì„ êº¼ë²„ë ¤ì„œ ëª¨ë¸ì´ íŠ¹ì • ë‰´ëŸ°ì— ê³¼í•˜ê²Œ ì˜ì¡´í•˜ì§€ ëª»í•˜ê²Œ ë§‰ì¤Œ
ì²« ë²ˆì§¸ Dense(128 ë‰´ëŸ°) ë’¤ì—ì„œ í•œ ë²ˆ
ë‘ ë²ˆì§¸ Dense(64 ë‰´ëŸ°) ë’¤ì—ì„œ í•œ ë²ˆ
2ë²ˆ ë„£ì€ ì´ìœ ëŠ”: "ë‘ ê°œì˜ í° Dense ë ˆì´ì–´ ê°ê°ì—ì„œ ê³¼ì í•©ì„ ë§‰ê¸° ìœ„í•´ì„œ"
<ìš”ì•½>
Dense ë ˆì´ì–´ë§ˆë‹¤ ë³µì¡ë„ê°€ ë†’ì•„ì§€ë‹ˆê¹Œ
ê³¼ì í•© ë§‰ìœ¼ë ¤ê³  Dropoutì„ ê° ë ˆì´ì–´ ë’¤ì— ë”°ë¡œ ë„£ì€ ê²ƒ
(íŠ¹íˆ 0.3ì€ ê½¤ ê°•í•˜ê²Œ ë–¨ì–´ëœ¨ë¦¬ëŠ” í¸)

2. SoftmaxëŠ” ì–´ë–¤ ì›ë¦¬ë¡œ í´ë˜ìŠ¤ í•˜ë‚˜ë¥¼ ì„ íƒ?
ğŸ‘‰ ì¶œë ¥ê°’(ë¡œì§“)ì„ í™•ë¥ ì²˜ëŸ¼ ë³€í™˜í•´ì„œ, ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ì„ íƒ
ë””í…Œì¼í•˜ê²Œ ë³´ë©´:
ë§ˆì§€ë§‰ Dense ì¸µì€ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ "ë¡œì§“ (logit)"ì„ ì¶œë ¥ (ê·¸ëƒ¥ ì ìˆ˜ ê°™ì€ ê°’)
SoftmaxëŠ” ì´ ë¡œì§“ë“¤ì„ "í™•ë¥  ë¶„í¬"ì²˜ëŸ¼ ë³€í™˜
ìˆ˜ì‹ì„ í†µí•´ ê³„ì‚° í›„ np.argmaxë¥¼ ì¨ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ìµœì¢… ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì„ íƒ

3. EarlyStoppingì€ ì–´ë–»ê²Œ í•™ìŠµì„ ë©ˆì¶”ëŠ” ê¸°ì¤€ì„ ì¡ëŠ”ê±¸ê¹Œ?
ğŸ‘‰ validation ë°ì´í„°ì— ëŒ€í•´ ë” ì´ìƒ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ì§€ ì•Šìœ¼ë©´ í•™ìŠµì„ ë©ˆì¶”ëŠ” ê¸°ìˆ ì´ì•¼.

êµ¬ì²´ì ìœ¼ë¡œ:
monitor='val_loss': validation setì— ëŒ€í•œ lossë¥¼ ì§€ì¼œë´„
patience=5: ë§Œì•½ 5ë²ˆ ì—°ì† validation lossê°€ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ë©ˆì¶¤
ì¦‰, "validation lossê°€ 5ë²ˆ ì—í­ ë™ì•ˆ ê³„ì† ì¤„ì§€ ì•Šìœ¼ë©´ â†’ ê³¼ì í•©ì´ ì‹œì‘ëë‹¤ê³  íŒë‹¨ â†’ í•™ìŠµ ì¤‘ë‹¨"
ì¶”ê°€ë¡œ, restore_best_weights=True ì˜µì…˜ ë•ë¶„ì—, ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ë˜ ì‹œì ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¡œ ìë™ìœ¼ë¡œ ë³µì›
"""
"""
Weighted F1 Score: 0.9901
Classification Report:
              precision    recall  f1-score   support

           0       0.98      1.00      0.99       663
           1       0.99      0.98      0.98       419
           2       1.00      0.99      1.00      2186
           3       0.86      0.63      0.73        19
           4       0.99      0.99      0.99       668
           5       0.93      0.93      0.93        45

    accuracy                           0.99      4000
   macro avg       0.96      0.92      0.94      4000
weighted avg       0.99      0.99      0.99      4000

1. ì „ì²´ ì •í™•ë„ (Accuracy)
99% (0.99)
â†’ 4000ê°œ ìƒ˜í”Œ ì¤‘ ê±°ì˜ ë‹¤ ë§ì·„ë‹¤ëŠ” ëœ»ì´ì•¼.

2. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥

í´ë˜ìŠ¤	precision	recall	f1-score	ìƒ˜í”Œ ê°œìˆ˜ (support)	ì„¤ëª…
0	0.98	1.00	0.99	663ê°œ	ê±°ì˜ ì™„ë²½í•˜ê²Œ ë¶„ë¥˜
1	0.99	0.98	0.98	419ê°œ	ì•„ì£¼ ì˜ ë¶„ë¥˜
2	1.00	0.99	1.00	2186ê°œ	ì‚¬ì‹¤ìƒ ì™„ë²½
3	0.86	0.63	0.73	19ê°œ	ì„±ëŠ¥ ë–¨ì–´ì§ âš¡
4	0.99	0.99	0.99	668ê°œ	ë§¤ìš° ì¢‹ìŒ
5	0.93	0.93	0.93	45ê°œ	ê´œì°®ìŒ
3. íŠ¹ë³„íˆ ì£¼ì˜í•  ë¶€ë¶„
í´ë˜ìŠ¤ 3 (supportê°€ 19ê°œë°–ì— ì•ˆ ë˜ëŠ” ì†Œìˆ˜ í´ë˜ìŠ¤)ì€ recallì´ 0.63ìœ¼ë¡œ ë‚®ì•„.
â†’ ì´ ë§ì€ ì‹¤ì œ 3ì¸ ë°ì´í„° ì¤‘ 63%ë§Œ ë§ì¶”ê³ , 37%ëŠ” ë†“ì³¤ë‹¤ëŠ” ëœ»ì´ì•¼.
â†’ ë°ì´í„° ìˆ˜ê°€ ì ê±°ë‚˜, ëª¨ë¸ì´ ì´ í´ë˜ìŠ¤ë¥¼ ì˜ í•™ìŠµ ëª»í–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆì–´.

4. í‰ê·  ì„±ëŠ¥
macro avg (ëª¨ë“  í´ë˜ìŠ¤ë¥¼ ë™ë“±í•˜ê²Œ ë³¸ í‰ê· )

precision: 0.96

recall: 0.92

f1-score: 0.94

weighted avg (ê° í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ê³ ë ¤í•œ í‰ê· )

ëª¨ë‘ 0.99ë¡œ ì•„ì£¼ ë†’ì•„.

âœ¨ ì´í‰
ëª¨ë¸ì€ ì „ë°˜ì ìœ¼ë¡œ ë§¤ìš° ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆì–´!

ë‹¨, í´ë˜ìŠ¤ 3 ê°œì„ ì„ ê³ ë¯¼í•´ë³´ëŠ” ê²Œ ì¢‹ì•„ ë³´ì—¬.

í´ë˜ìŠ¤ 3 ë°ì´í„° ìˆ˜ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜

í´ë˜ìŠ¤ 3ì— ê°€ì¤‘ì¹˜ë¥¼ ë” ì£¼ëŠ” ë°©ì‹ (class_weight="balanced" ê°™ì€ ì„¤ì •) ê³ ë ¤ ê°€ëŠ¥.
"""
# 6-1. ë”¥ëŸ¬ë‹ ëª¨ë¸ ë§Œë“¤ê¸°
nn_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(len(np.unique(y_encoded)), activation='softmax')  # í´ë˜ìŠ¤ ê°œìˆ˜ ë§Œí¼
])

nn_model.compile(optimizer='adam',
                 loss='sparse_categorical_crossentropy',
                 metrics=['accuracy'])

# 6-2. ë”¥ëŸ¬ë‹ í•™ìŠµ
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = nn_model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=32,
    callbacks=[early_stopping],
    verbose=1
)

# 6-3. ë”¥ëŸ¬ë‹ í‰ê°€
nn_preds = np.argmax(nn_model.predict(X_test), axis=1)

print("Neural Network Classification Report")
print(classification_report(y_test, nn_preds, target_names=le_body.classes_))

# 1. ëª¨ë¸ ì˜ˆì¸¡
y_pred_probs = nn_model.predict(X_test)
y_pred_classes = np.argmax(y_pred_probs, axis=1)  # í™•ë¥  -> í´ë˜ìŠ¤ ì„ íƒ
y_true_classes = y_test  # ê·¸ëƒ¥ y_test ì‚¬ìš©

# 2. Confusion Matrix ê³„ì‚°
cm = confusion_matrix(y_true_classes, y_pred_classes)

# 3. Confusion Matrix ì‹œê°í™”
plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=le_body.classes_, yticklabels=le_body.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# 4. Classification Report ì¶œë ¥
print("Classification Report:")
print(classification_report(y_true_classes, y_pred_classes, target_names=le_body.classes_))

# 5. F1 Score ì¶œë ¥
from sklearn.metrics import f1_score

f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')
print(f"Weighted F1 Score: {f1:.4f}")

# 6-4. ë”¥ëŸ¬ë‹ í˜¼ë™í–‰ë ¬ ì‹œê°í™”
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test, nn_preds), annot=True, fmt='d', cmap='Greens',
            xticklabels=le_body.classes_, yticklabels=le_body.classes_)
plt.title('Neural Network Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# 6-5. í•™ìŠµ ê³¼ì • ì‹œê°í™” (Loss, Accuracy)
plt.figure(figsize=(14,5))

# Loss
plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Loss Over Epochs')

# Accuracy
plt.subplot(1,2,2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title('Accuracy Over Epochs')

plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# 1. ëª¨ë¸ ì˜ˆì¸¡
y_pred_probs = nn_model.predict(X_test)
y_pred_classes = np.argmax(y_pred_probs, axis=1)  # í™•ë¥  -> í´ë˜ìŠ¤ ì„ íƒ
y_true_classes = y_test  # ì—¬ê¸°! ê·¸ëƒ¥ y_testë¥¼ ê·¸ëŒ€ë¡œ ì¨

# 2. Confusion Matrix ê³„ì‚°
cm = confusion_matrix(y_true_classes, y_pred_classes)

# 3. Confusion Matrix ì‹œê°í™”
plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# 4. Classification Report ì¶œë ¥
print("Classification Report:")
print(classification_report(y_true_classes, y_pred_classes))
