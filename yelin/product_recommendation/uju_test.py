import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.metrics import silhouette_score
import os

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs('plots', exist_ok=True)

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv('../../renttherunway_data.csv')
print(f"ì „ì²´ ë°ì´í„° í¬ê¸°: {df.shape}")

# ğŸ‘‰ ë°ì´í„° 50% ìƒ˜í”Œë§
# df = df.sample(frac=0.5, random_state=42).reset_index(drop=True)

# í‚¤ë¥¼ cmë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def height_to_cm(height):
    if pd.isna(height):
        return np.nan
    try:
        feet, inches = height.split("'")
        inches = inches.replace('"', '').strip()
        return int(feet) * 30.48 + int(inches) * 2.54
    except:
        return np.nan

# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocessing_data(data):
    selected_columns = ['user_id', 'item_id', 'fit', 'weight', 'rating', 'body type', 'height', 'size', 'age']
    data = data[selected_columns].copy()
    data = data[data['fit'] == 'fit'].copy()

    data['height_cm'] = data['height'].apply(height_to_cm)
    data['weight'] = data['weight'].astype(str).str.replace('lbs', '').str.strip()
    data['weight'] = pd.to_numeric(data['weight'], errors='coerce')
    data['weight_kg'] = data['weight'] * 0.453592

    data['rating'] = pd.to_numeric(data['rating'], errors='coerce').replace([np.inf, -np.inf], np.nan)
    rating_median = data['rating'].median()
    data['rating'] = data['rating'].fillna(rating_median)
    data['rating_5'] = (data['rating'] / 2).round().astype(int).clip(1, 5)

    numeric_columns = ['height_cm', 'weight_kg', 'rating_5', 'age', 'size']
    for col in numeric_columns:
        data[col] = data[col].fillna(data[col].mean())

    categorical_columns = ['fit', 'body type']
    for col in categorical_columns:
        data[col] = data[col].fillna(data[col].mode()[0])

    # ì›-í•« ì¸ì½”ë”©
    # encoder_fit = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    # fit_encoded = pd.DataFrame(encoder_fit.fit_transform(data[['fit']]),
    #                            columns=encoder_fit.get_feature_names_out(['fit']), index=data.index)

    encoder_body = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    body_encoded = pd.DataFrame(encoder_body.fit_transform(data[['body type']]),
                                columns=encoder_body.get_feature_names_out(['body type']), index=data.index)
    # data = pd.concat([data, fit_encoded, body_encoded], axis=1)
    data = pd.concat([data, body_encoded], axis=1)
    return data

# ìŠ¤ì¼€ì¼ë§ í•¨ìˆ˜
def scaling_data(data, onehot_weight=0.2):
    scaler = MinMaxScaler()
    scaled_columns = ['weight_kg', 'height_cm', 'size', 'age']

    # ì—°ì†í˜• ë³€ìˆ˜ ìŠ¤ì¼€ì¼ë§
    for col in scaled_columns:
        data[f'{col}_scaled'] = scaler.fit_transform(data[[col]])

    # ì›í•« ì¸ì½”ë”© ì»¬ëŸ¼ ì°¾ê¸°
    onehot_columns = [col for col in data.columns if col.startswith(('fit_', 'body type_'))]

    # ì›í•« ì¸ì½”ë”© ì»¬ëŸ¼ì— ê°€ì¤‘ì¹˜ ì ìš©
    for col in onehot_columns:
        data[col] = data[col] / onehot_weight

    # ìµœì¢… feature ëª¨ìŒ
    feature_columns = [f'{col}_scaled' for col in scaled_columns] + onehot_columns
    return data[feature_columns]


def find_auto_onehot_weight(train_df, optimal_k=20):
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score

    # ìŠ¤ì¼€ì¼ë§ + ì—°ì†í˜• ë³€ìˆ˜ í‰ê·  ê³„ì‚°
    scaled_data = train_df.copy()
    scaler = MinMaxScaler()
    scaled_columns = ['weight_kg', 'height_cm', 'size', 'age']

    for col in scaled_columns:
        scaled_data[f'{col}_scaled'] = scaler.fit_transform(scaled_data[[col]])

    # ì—°ì†í˜• ìŠ¤ì¼€ì¼ë§ëœ ì»¬ëŸ¼ë“¤ì˜ í‰ê·  ê³„ì‚°
    feature_means = []
    for col in scaled_columns:
        mean_value = scaled_data[f'{col}_scaled'].mean()
        feature_means.append(mean_value)

    auto_onehot_weight = np.mean(feature_means)
    print(f"\nğŸ“ˆ ìë™ ê³„ì‚°ëœ OneHot ê°€ì¤‘ì¹˜ (ì—°ì†í˜• í‰ê·  ê¸°ë°˜): {auto_onehot_weight:.4f}")

    # ì´ì œ ì´ ê°€ì¤‘ì¹˜ë¡œ ë‹¤ì‹œ ìŠ¤ì¼€ì¼ë§
    onehot_columns = [col for col in scaled_data.columns if col.startswith(('fit_', 'body type_'))]
    for col in onehot_columns:
        scaled_data[col] = scaled_data[col] * auto_onehot_weight

    feature_columns = [f'{col}_scaled' for col in scaled_columns] + onehot_columns
    feature_df = scaled_data[feature_columns]

    # KMeans í•™ìŠµ
    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10, init='k-means++')
    labels = kmeans.fit_predict(feature_df)

    # ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
    silhouette = silhouette_score(feature_df, labels)
    print(f"âœ… ì‹¤ë£¨ì—£ ì ìˆ˜ (auto weight): {silhouette:.4f}")

    return auto_onehot_weight, silhouette

# ë°ì´í„° ë¶„í•  í•¨ìˆ˜
def train_new_data_split(data, test_size, random_state):
    train_data, new_data = train_test_split(data, test_size=test_size, random_state=random_state)
    print(f"train í¬ê¸°: {train_data.shape}, new í¬ê¸°: {new_data.shape}")
    return train_data, new_data

# ìµœì ì˜ kì°¾ì•„ì„œ ì ìš©
def find_optimal_k(train_features_df, k_min=2, k_max=30):
    wcss = []
    silhouette_scores = []

    for k in range(k_min, k_max + 1):
        """
        KMeans(n_clusters=optimal_k,
                random_state=42,
                n_init=10,
                init='k-means++',
                max_iter=500,
                algorithm='elkan',
                tol=1e-5)
        """
        # kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, init='k-means++')
        kmeans = KMeans(n_clusters=k,
               random_state=42,
               n_init=10,
               init='k-means++',
               max_iter=500,
               algorithm='elkan',
               tol=1e-5)
        labels = kmeans.fit_predict(train_features_df)
        wcss.append(kmeans.inertia_)
        if k > 1:  # ì‹¤ë£¨ì—£ ì ìˆ˜ëŠ” k=2 ì´ìƒë¶€í„° ì˜ë¯¸ ìˆìŒ
            silhouette = silhouette_score(train_features_df, labels)
            silhouette_scores.append((k, silhouette))
            print(k,"ì‹¤ë£¨ì—£ ì ìˆ˜: ", silhouette)

    # ì—˜ë³´ìš° í”Œë¡¯ ì €ì¥
    plt.figure(figsize=(10, 6))
    plt.plot(range(k_min, k_max + 1), wcss, marker='o')
    plt.title('Elbow Method for Optimal K')
    plt.xlabel('Number of clusters (K)')
    plt.ylabel('WCSS')
    plt.savefig('plots/elbow_plot.png')
    plt.show()
    plt.close()

    # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ê°€ ê°€ì¥ ë†’ì€ k ì„ íƒ
    best_k, best_silhouette = max(silhouette_scores, key=lambda x: x[1])
    print(f"\nâœ… ìë™ìœ¼ë¡œ ì„ íƒëœ ìµœì  K: {best_k} (Silhouette: {best_silhouette:.4f})")
    return best_k


from sklearn.decomposition import PCA


def pca_based_onehot_weight(train_df, continuous_cols, onehot_cols):
    # ì—°ì†í˜•, ë²”ì£¼í˜• êµ¬ë¶„
    cont_data = train_df[continuous_cols]
    onehot_data = train_df[onehot_cols]

    # ì—°ì†í˜• ìŠ¤ì¼€ì¼ë§
    scaler = MinMaxScaler()
    cont_scaled = scaler.fit_transform(cont_data)

    # ì›-í•«ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    data_combined = np.hstack([cont_scaled, onehot_data.values])

    # PCA ì ìš©
    pca = PCA(n_components=min(data_combined.shape))
    pca.fit(data_combined)

    # ê° ë³€ìˆ˜ë³„ ê¸°ì—¬ë„
    loading_scores = np.abs(pca.components_).mean(axis=0)  # ê° featureë³„ í‰ê·  loading

    # ì—°ì†í˜•, ë²”ì£¼í˜• ë¶„ë¦¬
    cont_loading = loading_scores[:len(continuous_cols)].mean()
    onehot_loading = loading_scores[len(continuous_cols):].mean()

    # ë¹„ìœ¨ ê³„ì‚°
    weight_ratio = cont_loading / onehot_loading
    print(f"ì—°ì†í˜• í‰ê·  Loading: {cont_loading:.4f}, ë²”ì£¼í˜• í‰ê·  Loading: {onehot_loading:.4f}")
    print(f"ì¶”ì²œí•˜ëŠ” ë²”ì£¼í˜• ê°€ì¤‘ì¹˜ (PCA ê¸°ë°˜): {weight_ratio:.4f}")

    return weight_ratio


# --- ë¦¬íŒ©í† ë§ ì‹¤í–‰ íë¦„ ì‹œì‘ ---
"""
1. ë¯¸ë¦¬ ì „ì²˜ë¦¬í•˜ê³  ë‚˜ëˆ„ëŠ” ê²½ìš°

â¡ï¸ ì „ì²˜ë¦¬(ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°, ìŠ¤ì¼€ì¼ë§ ë“±)ë¥¼ ì „ì²´ ë°ì´í„°ì— ì ìš©í•˜ê³  ë‚˜ëˆ”.

â¡ï¸ ìƒˆ ë°ì´í„°ê°€ í›ˆë ¨ ë°ì´í„° ì •ë³´ë¥¼ ì¼ë¶€ 'ì—¿ë³¸' ìƒíƒœê°€ ë  ìˆ˜ ìˆìŒ.

â¡ï¸ íŠ¹íˆ í‰ê· , ì¤‘ì•™ê°’, ìµœë¹ˆê°’ì„ ì±„ìš¸ ë•Œ ì „ì²´ í‰ê· ì„ ì‚¬ìš©í•˜ë©´,

ì‹¤ì œ ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ì •ë³´ë¥¼ ì´ë¯¸ ì¨ë²„ë¦° ê²Œ ë¼.

(â†’ í˜„ì‹¤ì—ì„œëŠ” ì‹¤ì œë¡œ "ìƒˆ ë°ì´í„°"ëŠ” ìš°ë¦¬ê°€ ì•„ì§ ëª¨ë¥´ëŠ” ìƒíƒœë¡œ ë“¤ì–´ì˜¤ì–ì•„?
ê·¸ëŸ° ìƒí™©ì„ ì—°ìŠµí•˜ë ¤ê³  ì´ë ‡ê²Œ ì—„ê²©í•˜ê²Œ ê´€ë¦¬í•˜ëŠ” ê±°ì•¼.)

2. ê°ê° ì „ì²˜ë¦¬í•˜ëŠ” ê²½ìš° (ë‚´ê°€ í•œ ë°©ë²•)

â¡ï¸ train/test ë‚˜ëˆˆ ë‹¤ìŒ, trainì—ì„œë§Œ í‰ê· /ì¤‘ì•™ê°’/ìµœë¹ˆê°’ ê³„ì‚°.

â¡ï¸ test(new) ë°ì´í„°ëŠ” í›ˆë ¨ ë°ì´í„°ë¡œ ë§Œë“¤ì–´ì§„ ê¸°ì¤€ì— ë§ì¶° ì ìš©.

â¡ï¸ ì§„ì§œë¡œ "ëª¨ë¥´ëŠ” ë°ì´í„°"ë¥¼ ë‹¤ë£¨ëŠ” ê²ƒì²˜ëŸ¼ í›ˆë ¨í•˜ê³  í‰ê°€í•  ìˆ˜ ìˆì–´.
"""
# 1. ë°ì´í„° ë¶„í•  (ì›ë³¸ì—ì„œ ë¨¼ì €!)
train_df_raw, new_df_raw = train_new_data_split(df, test_size=0.01, random_state=42)

# 2. ê°ê° ì „ì²˜ë¦¬
train_df = preprocessing_data(train_df_raw)
new_df = preprocessing_data(new_df_raw)

# ğŸ”¥ PCA ê¸°ë°˜ìœ¼ë¡œ onehot ê°€ì¤‘ì¹˜ ê³„ì‚°
# continuous_cols = ['height_cm', 'weight_kg', 'rating_5', 'age', 'size']  # ì—°ì†í˜• ì»¬ëŸ¼
# onehot_cols = [col for col in train_df.columns if col.startswith(('fit_', 'body type_'))]  # ì›í•« ì»¬ëŸ¼
# pca_weight = pca_based_onehot_weight(train_df, continuous_cols, onehot_cols)
# print(pca_weight)

# í›„ë³´ ê°€ì¤‘ì¹˜ ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°
# weight_candidates = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 1.0]

# ìµœì  ê°€ì¤‘ì¹˜ ì°¾ê¸°
# best_weight, all_scores = find_best_onehot_weight(train_df, weight_candidates, optimal_k=20)

# ìë™ìœ¼ë¡œ onehot ê°€ì¤‘ì¹˜ ê³„ì‚°
# auto_weight, auto_score = find_auto_onehot_weight(train_df, optimal_k=20)

# ì´ auto_weightë¥¼ scaling_data()ì— ë„˜ê²¨ì„œ ìµœì¢… ë°ì´í„° ì¤€ë¹„
train_features_df = scaling_data(train_df, onehot_weight=0.7989)
new_features_df = scaling_data(new_df, onehot_weight=0.7989)

# best_weightë¥¼ scaling_data()ì— ë„˜ê²¨ì„œ ìµœì¢… ë°ì´í„° ì¤€ë¹„í•˜ê¸°
# train_features_df = scaling_data(train_df, onehot_weight=best_weight)
# new_features_df = scaling_data(new_df, onehot_weight=best_weight)

# 3. ê°ê° ìŠ¤ì¼€ì¼ë§
# train_features_df = scaling_data(train_df)
# new_features_df = scaling_data(new_df)

# 4. ì—˜ë³´ìš° ê¸°ë²•ìœ¼ë¡œ ìµœì  í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
# wcss = []
# for k in range(20, 31):
#     kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, init='k-means++')
#     kmeans.fit(train_features_df)
#     wcss.append(kmeans.inertia_)
#
# plt.figure(figsize=(10, 6))
# plt.plot(range(1, 11), wcss, marker='o')
# plt.title('Elbow Method for Optimal K')
# plt.xlabel('Number of clusters (K)')
# plt.ylabel('WCSS')
# plt.savefig('plots/elbow_plot.png')
# plt.show()
# plt.close()

# 4. ìµœì  í´ëŸ¬ìŠ¤í„° ì°¾ê¸° (ìë™)
# optimal_k = find_optimal_k(train_features_df, k_min=6, k_max=21)
optimal_k = 21

# 5. K-Means í´ëŸ¬ìŠ¤í„°ë§
# optimal_k = 20
# kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10, init='k-means++')
# train_df['cluster'] = kmeans.fit_predict(train_features_df)

# 5. K-Means í´ëŸ¬ìŠ¤í„°ë§
kmeans = KMeans(n_clusters=optimal_k,
                random_state=42,
                n_init=10,
                init='k-means++',
                max_iter=500,
                algorithm='elkan',
                tol=1e-5)
train_df['cluster'] = kmeans.fit_predict(train_features_df)

# 6. í•™ìŠµ ë°ì´í„° ì‹¤ë£¨ì—£ ì ìˆ˜
train_silhouette = silhouette_score(train_features_df, train_df['cluster'])
print(f"\ní•™ìŠµ ë°ì´í„° ì‹¤ë£¨ì—£ ì ìˆ˜: {train_silhouette:.3f}")

# 7. êµ°ì§‘ë³„ íŠ¹ì„± ë¶„ì„
original_columns = ['weight_kg', 'height_cm', 'size', 'age', 'rating_5']
print("\ní•™ìŠµ ë°ì´í„° êµ°ì§‘ë³„ í‰ê· :")
print(train_df.groupby('cluster')[original_columns].mean())

# 8. ì¶”ì²œ ìƒí’ˆ ì„ ì •
cluster_items = train_df.groupby('cluster').apply(
    lambda x: x.nlargest(5, 'rating_5')[['item_id', 'rating_5']].to_dict('records')
)

print("\nêµ°ì§‘ë³„ ìƒìœ„ 5ê°œ ì¶”ì²œ ìƒí’ˆ:")
for cluster, items in cluster_items.items():
    print(f"Cluster {cluster}:")
    for item in items:
        print(f"  Item ID: {item['item_id']}, Rating: {item['rating_5']}")

# 9. ìƒˆë¡œìš´ ë°ì´í„° êµ°ì§‘ ì˜ˆì¸¡
new_df['cluster'] = kmeans.predict(new_features_df)

print("\nìƒˆë¡œìš´ ë°ì´í„° êµ°ì§‘ ë¶„í¬:")
print(new_df['cluster'].value_counts())

# 10. ìƒˆë¡œìš´ ë°ì´í„° ì‹¤ë£¨ì—£ ì ìˆ˜
new_silhouette = silhouette_score(new_features_df, new_df['cluster'])
print(f"\nìƒˆë¡œìš´ ë°ì´í„° ì‹¤ë£¨ì—£ ì ìˆ˜: {new_silhouette:.3f}")


# 11. Precision, Recall, F1 í‰ê°€
def evaluate_recommendations(df, cluster_items, K=5):
    precision_scores = []
    recall_scores = []

    for cluster in df['cluster'].unique():
        real_items = set(df[(df['cluster'] == cluster) & (df['rating_5'] >= 4)]['item_id'])
        recommended_items = set(item['item_id'] for item in cluster_items.get(cluster, []))
        hits = len(real_items & recommended_items)
        precision = hits / K if K > 0 else 0
        recall = hits / len(real_items) if real_items else 0
        precision_scores.append(precision)
        recall_scores.append(recall)

    f1_scores = [2 * (p * r) / (p + r) if (p + r) > 0 else 0 for p, r in zip(precision_scores, recall_scores)]
    return np.mean(precision_scores), np.mean(recall_scores), np.mean(f1_scores)

train_precision, train_recall, train_f1 = evaluate_recommendations(train_df, cluster_items)
new_precision, new_recall, new_f1 = evaluate_recommendations(new_df, cluster_items)

print(f"\ní•™ìŠµ ë°ì´í„° Precision@5: {train_precision:.3f}, Recall@5: {train_recall:.3f}, F1 Score: {train_f1:.3f}")
print(f"ìƒˆë¡œìš´ ë°ì´í„° Precision@5: {new_precision:.3f}, Recall@5: {new_recall:.3f}, F1 Score: {new_f1:.3f}")

# 12. ìƒˆë¡œìš´ ë°ì´í„° ì¶”ì²œ í’ˆì§ˆ (Feedback Score)
feedback_scores = []
for cluster in new_df['cluster'].unique():
    recommended_items = set(item['item_id'] for item in cluster_items.get(cluster, []))
    cluster_data = new_df[(new_df['cluster'] == cluster) & (new_df['item_id'].isin(recommended_items))]
    if not cluster_data.empty:
        feedback_score = (cluster_data['rating_5'] >= 4).mean()
        feedback_scores.append(feedback_score)
    else:
        feedback_scores.append(0)

print(f"\nìƒˆë¡œìš´ ë°ì´í„° ì¶”ì²œ ìƒí’ˆì˜ ë†’ì€ í‰ì  ë¹„ìœ¨ (í‰ê· ): {np.mean(feedback_scores):.3f}")

# 13. ìƒˆë¡œìš´ ë°ì´í„° ì¶”ì²œ ìƒí’ˆ ì¶œë ¥
print("\nìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ êµ°ì§‘ë³„ ì¶”ì²œ ìƒí’ˆ:")
for cluster in new_df['cluster'].unique():
    print(f"Cluster {cluster} ì¶”ì²œ ìƒí’ˆ:")
    for item in cluster_items.get(cluster, []):
        print(f"  Item ID: {item['item_id']}, Rating: {item['rating_5']}")
